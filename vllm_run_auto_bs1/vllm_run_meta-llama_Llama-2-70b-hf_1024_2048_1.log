INFO 08-15 11:58:29 [__init__.py:243] Automatically detected platform cuda.
benchmarking meta-llama_Llama-2-70b-hf_prompt1024_gen2048_bs1
kv cache size per request: 7.5 GB
offloading 140 GB model weights
INFO 08-15 11:58:33 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-15 11:58:33 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-15 11:58:33 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-15 11:58:43 [config.py:793] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 08-15 11:58:43 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.2.dev1+g0755d3cc1) with config: model='meta-llama/Llama-2-70b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-70b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.DUMMY, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=meta-llama/Llama-2-70b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [32, 24, 16, 8, 4, 2, 1], "max_capture_size": 32}, use_cached_outputs=False, 
INFO 08-15 11:58:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 08-15 11:58:46 [cuda.py:289] Using XFormers backend.
INFO 08-15 11:58:46 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-15 11:58:46 [model_runner.py:1170] Starting to load model meta-llama/Llama-2-70b-hf...
INFO 08-15 12:08:20 [model_runner.py:1202] Model loading took 0.9776 GiB and 572.960796 seconds
INFO 08-15 12:08:39 [worker.py:291] Memory profiling takes 19.05 seconds
INFO 08-15 12:08:39 [worker.py:291] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB
INFO 08-15 12:08:39 [worker.py:291] model weights take 0.98GiB; non_torch_memory takes 0.38GiB; PyTorch activation peak memory takes 2.23GiB; the rest of the memory reserved for KV Cache is 10.60GiB.
INFO 08-15 12:08:39 [executor_base.py:112] # cuda blocks: 2170, # CPU blocks: 0
INFO 08-15 12:08:39 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 11.30x
INFO 08-15 12:08:39 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/7 [00:00<?, ?it/s]Capturing CUDA graph shapes:  14%|█▍        | 1/7 [00:25<02:32, 25.44s/it]Capturing CUDA graph shapes:  29%|██▊       | 2/7 [00:50<02:06, 25.34s/it]Capturing CUDA graph shapes:  43%|████▎     | 3/7 [01:16<01:41, 25.33s/it]Capturing CUDA graph shapes:  57%|█████▋    | 4/7 [01:41<01:15, 25.30s/it]Capturing CUDA graph shapes:  71%|███████▏  | 5/7 [02:06<00:50, 25.27s/it]Capturing CUDA graph shapes:  86%|████████▌ | 6/7 [02:31<00:25, 25.26s/it]Capturing CUDA graph shapes: 100%|██████████| 7/7 [02:56<00:00, 25.26s/it]Capturing CUDA graph shapes: 100%|██████████| 7/7 [02:56<00:00, 25.28s/it]
INFO 08-15 12:11:36 [model_runner.py:1670] Graph capturing finished in 177 secs, took 1.71 GiB
INFO 08-15 12:11:36 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 196.57 seconds
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 119, in <module>
[rank0]:     benchmark(args.model, args.prompt_len, args.gen_len, args.batch_size, args.tensor_parallelism)
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 67, in benchmark
[rank0]:     responses = llm.generate(prompts, sampling_params)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 1218, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/entrypoints/llm.py", line 481, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/entrypoints/llm.py", line 1476, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 1393, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/executor_base.py", line 140, in execute_model
[rank0]:     output = self.collective_rpc("execute_model",
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 2605, in run_method
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/worker_base.py", line 420, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/model_runner.py", line 1843, in execute_model
[rank0]:     hidden_or_intermediate_states = model_executable(
[rank0]:                                     ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 580, in forward
[rank0]:     model_output = self.model(input_ids, positions, intermediate_tensors,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/compilation/decorators.py", line 172, in __call__
[rank0]:     return self.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 391, in forward
[rank0]:     hidden_states, residual = layer(positions, hidden_states, residual)
[rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/utils.py", line 599, in forward
[rank0]:     output = functional_call(module,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/_functorch/functional_call.py", line 148, in functional_call
[rank0]:     return nn.utils.stateless._functional_call(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/utils/stateless.py", line 282, in _functional_call
[rank0]:     return module(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 304, in forward
[rank0]:     hidden_states = self.self_attn(positions=positions,
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 202, in forward
[rank0]:     attn_output = self.attn(q, k, v)
[rank0]:                   ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/attention/layer.py", line 237, in forward
[rank0]:     return torch.ops.vllm.unified_attention(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/_ops.py", line 1158, in __call__
[rank0]:     return self._op(*args, **(kwargs or {}))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/attention/layer.py", line 386, in unified_attention
[rank0]:     output = self.impl.forward(self, query, key, value, kv_cache,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/attention/backends/xformers.py", line 563, in forward
[rank0]:     out = self._run_memory_efficient_xformers_forward(
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/attention/backends/xformers.py", line 736, in _run_memory_efficient_xformers_forward
[rank0]:     out = xops.memory_efficient_attention_forward(
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/xformers/ops/fmha/__init__.py", line 376, in memory_efficient_attention_forward
[rank0]:     return _memory_efficient_attention_forward(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/xformers/ops/fmha/__init__.py", line 490, in _memory_efficient_attention_forward
[rank0]:     out, *_ = op.apply(inp, needs_gradient=False)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/xformers/ops/fmha/cutlass.py", line 250, in apply
[rank0]:     cls.apply_bmhk(
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/xformers/ops/fmha/cutlass.py", line 273, in apply_bmhk
[rank0]:     out, lse, rng_seed, rng_offset, _, _ = cls.OPERATOR(
[rank0]:                                            ^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/_ops.py", line 1158, in __call__
[rank0]:     return self._op(*args, **(kwargs or {}))
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.47 GiB memory in use. Of the allocated memory 14.90 GiB is allocated by PyTorch, with 1.62 GiB allocated in private pools (e.g., CUDA Graphs), and 115.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W815 12:11:37.112420979 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
