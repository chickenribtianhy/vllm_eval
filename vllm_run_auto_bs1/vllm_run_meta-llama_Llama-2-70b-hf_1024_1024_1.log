INFO 08-15 12:16:31 [__init__.py:243] Automatically detected platform cuda.
benchmarking meta-llama_Llama-2-70b-hf_prompt1024_gen1024_bs1
kv cache size per request: 5.0 GB
offloading 140 GB model weights
INFO 08-15 12:16:33 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-15 12:16:33 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-15 12:16:33 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-15 12:16:41 [config.py:793] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 08-15 12:16:41 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.2.dev1+g0755d3cc1) with config: model='meta-llama/Llama-2-70b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-70b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.DUMMY, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=meta-llama/Llama-2-70b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [32, 24, 16, 8, 4, 2, 1], "max_capture_size": 32}, use_cached_outputs=False, 
INFO 08-15 12:16:42 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 08-15 12:16:42 [cuda.py:289] Using XFormers backend.
INFO 08-15 12:16:43 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-15 12:16:43 [model_runner.py:1170] Starting to load model meta-llama/Llama-2-70b-hf...
INFO 08-15 12:26:14 [model_runner.py:1202] Model loading took 0.9773 GiB and 570.663443 seconds
INFO 08-15 12:26:31 [worker.py:291] Memory profiling takes 17.26 seconds
INFO 08-15 12:26:31 [worker.py:291] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.19GiB
INFO 08-15 12:26:31 [worker.py:291] model weights take 0.98GiB; non_torch_memory takes 0.38GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 10.81GiB.
INFO 08-15 12:26:32 [executor_base.py:112] # cuda blocks: 2213, # CPU blocks: 0
INFO 08-15 12:26:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 17.29x
INFO 08-15 12:26:32 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graph shapes:  14%|█▍        | 1/7 [00:25<02:32, 25.34s/it]
Capturing CUDA graph shapes:  14%|█▍        | 1/7 [00:25<02:32, 25.38s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 119, in <module>
[rank0]:     benchmark(args.model, args.prompt_len, args.gen_len, args.batch_size, args.tensor_parallelism)
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 46, in benchmark
[rank0]:     llm = LLM(model=model_name,
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 1183, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/entrypoints/llm.py", line 253, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 501, in from_engine_args
[rank0]:     return engine_cls.from_vllm_config(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 477, in from_vllm_config
[rank0]:     return cls(
[rank0]:            ^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 268, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 426, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/executor_base.py", line 123, in initialize_cache
[rank0]:     self.collective_rpc("initialize_cache",
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 2605, in run_method
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/worker.py", line 332, in initialize_cache
[rank0]:     self._warm_up_model()
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/worker.py", line 362, in _warm_up_model
[rank0]:     self.model_runner.capture_model(self.gpu_cache)
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/model_runner.py", line 1657, in capture_model
[rank0]:     graph_runner.capture(**capture_inputs)
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/model_runner.py", line 2059, in capture
[rank0]:     self.model(
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 580, in forward
[rank0]:     model_output = self.model(input_ids, positions, intermediate_tensors,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/compilation/decorators.py", line 172, in __call__
[rank0]:     return self.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 391, in forward
[rank0]:     hidden_states, residual = layer(positions, hidden_states, residual)
[rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/utils.py", line 596, in forward
[rank0]:     k: v.to(device, non_blocking=True)
[rank0]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 234.19 MiB is free. Including non-PyTorch memory, this process has 15.24 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, with 1.62 GiB allocated in private pools (e.g., CUDA Graphs), and 277.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W815 12:26:57.689807493 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
