INFO 08-15 10:59:09 [__init__.py:243] Automatically detected platform cuda.
benchmarking meta-llama_Llama-2-70b-hf_prompt1024_gen8192_bs1
kv cache size per request: 22.5 GB
offloading 0 GB model weights
INFO 08-15 10:59:13 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 08-15 10:59:13 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 08-15 10:59:13 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 08-15 10:59:25 [config.py:793] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 08-15 10:59:25 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.2.dev1+g0755d3cc1) with config: model='meta-llama/Llama-2-70b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-70b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.DUMMY, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=meta-llama/Llama-2-70b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [32, 24, 16, 8, 4, 2, 1], "max_capture_size": 32}, use_cached_outputs=False, 
INFO 08-15 10:59:28 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 08-15 10:59:28 [cuda.py:289] Using XFormers backend.
INFO 08-15 10:59:29 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-15 10:59:29 [model_runner.py:1170] Starting to load model meta-llama/Llama-2-70b-hf...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 119, in <module>
[rank0]:     benchmark(args.model, args.prompt_len, args.gen_len, args.batch_size, args.tensor_parallelism)
[rank0]:   File "/home/ubuntu/vllm_eval/main.py", line 46, in benchmark
[rank0]:     llm = LLM(model=model_name,
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 1183, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/entrypoints/llm.py", line 253, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 501, in from_engine_args
[rank0]:     return engine_cls.from_vllm_config(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 477, in from_vllm_config
[rank0]:     return cls(
[rank0]:            ^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/engine/llm_engine.py", line 265, in __init__
[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config)
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/executor_base.py", line 52, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/uniproc_executor.py", line 47, in _init_executor
[rank0]:     self.collective_rpc("load_model")
[rank0]:   File "/home/ubuntu/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/utils.py", line 2605, in run_method
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/worker.py", line 207, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/home/ubuntu/vllm/vllm/worker/model_runner.py", line 1173, in load_model
[rank0]:     self.model = get_model(vllm_config=self.vllm_config)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
[rank0]:     return loader.load_model(vllm_config=vllm_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/model_loader/dummy_loader.py", line 31, in load_model
[rank0]:     model = initialize_model(vllm_config=vllm_config)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
[rank0]:     return model_class(vllm_config=vllm_config, prefix=prefix)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 520, in __init__
[rank0]:     self.model = self._init_model(vllm_config=vllm_config,
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 566, in _init_model
[rank0]:     return LlamaModel(vllm_config=vllm_config,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/compilation/decorators.py", line 151, in __init__
[rank0]:     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 345, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:                                                     ^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/utils.py", line 626, in make_layers
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 347, in <lambda>
[rank0]:     lambda prefix: layer_type(config=config,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 278, in __init__
[rank0]:     self.mlp = LlamaMLP(
[rank0]:                ^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/models/llama.py", line 71, in __init__
[rank0]:     self.gate_up_proj = MergedColumnParallelLinear(
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/layers/linear.py", line 546, in __init__
[rank0]:     super().__init__(input_size=input_size,
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/layers/linear.py", line 410, in __init__
[rank0]:     self.quant_method.create_weights(
[rank0]:   File "/home/ubuntu/vllm/vllm/model_executor/layers/linear.py", line 189, in create_weights
[rank0]:     weight = Parameter(torch.empty(sum(output_partition_sizes),
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 298.19 MiB is free. Including non-PyTorch memory, this process has 15.47 GiB memory in use. Of the allocated memory 15.11 GiB is allocated by PyTorch, and 701.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W815 10:59:30.728170490 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
