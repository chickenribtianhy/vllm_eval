# vLLM Eval

vLLM offline inference evaluation

## Get Started

Create a new Python environment, requireing Python: 3.9 -- 3.12 \
Install vLLM with pip:

```bash
pip install vllm
```

Visit vLLM [documentation](https://docs.vllm.ai/) to learn more.

- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

To run vllm_eval, execute the provided shell script:
```bash
./run_eval.sh
```
Make sure the script has executable permissions. If needed, run:
```bash
chmod +x run_eval.sh
```
Runtime logs go to 
Calculated metrics go to 